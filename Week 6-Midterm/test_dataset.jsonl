{"questions": {"2f747c80-2d00-4016-85a8-a0c1e76b3703": "What procedures should be established for escalating GAI system incidents to the organizational risk management authority?", "f869267f-945d-41e3-b342-2304e04c057d": "How often should the specific criteria for the deactivation of GAI systems be reviewed?", "c2ef8666-75a2-42b1-9d3a-e8e418efcec1": "What are the key tasks involved in AI Actor management according to the provided context?", "e45f9562-6623-4a00-be37-9b9d5fbfbb69": "How are AI risks and benefits from third-party resources monitored and controlled?", "76919887-444a-4957-9eea-e4889495d3f4": "What are the limitations of current pre-deployment testing approaches for GAI applications?", "f87cfeb7-1602-4d60-be72-057e8e1bda33": "How do organizations measure performance and risks during the pre-deployment testing of GAI systems?", "f09a8cdc-9835-49f6-9172-a7f0a0d38240": "What roles do data providers and system funders play in this category?", "fc5d5742-0f43-4817-9de4-45be387d06be": "How are products related to the context of data providers and system funders?", "11b8d4bf-83e1-426c-8ef4-e6ef4fd271f8": "How does alignment to organizational values influence auditing and assessment processes?", "da4ccead-3cfc-4260-9b68-e4f492505a25": "What role does data provenance play in change-management controls?", "d364fd5d-6cdd-486d-b55b-35a2ce00dca5": "What are the main techniques used in provenance data tracking for GAI systems?", "0599add6-1dc0-4741-9520-3dc8f324292f": "How does provenance data tracking assist AI Actors throughout the lifecycle of digital content?", "a531e7c4-5222-4f61-9346-0d8717f462fb": "What types of metadata can be included in provenance tracking for GAI content?", "504213b1-86dc-402c-88e9-ecbc4c62dddf": "How can provenance data tracking techniques assist in assessing the authenticity of digital content?", "63023e75-06fc-4be6-8964-fe16f89ae3ef": "What are the primary goals of field testing in relation to AI-generated information?", "375c72a7-651d-4c66-8c79-03278dee8c86": "How does AI red-teaming contribute to identifying flaws in an AI system?", "f363b703-0ff8-4ba2-ae8b-3e7a2d511c47": "What are some implications of using third-party GAI models and systems for an organization?", "3e96af30-e33a-49f5-b066-1346e4d80b97": "How do existing governance protocols need to be adapted for GAI contexts?", "ed60263f-eabd-42bb-a5fa-c2f61e88c0a0": "What is the main focus of the paper by Shevlane et al (2023) regarding model evaluation?", "407e0915-d7f2-4ac1-a348-0ee545587238": "How do Shumailov et al (2023) describe the impact of training on generated data in their research?", "a39abfd0-b757-4be0-b3aa-261af691ac23": "What types of harmful content should be analyzed during the due diligence process for GAI output?", "077279e1-81ca-42dc-8f4d-a87e9a2aed9d": "How does the due diligence process address potential misinformation related to CBRN information or capabilities?", "55aa2480-8133-459a-9df1-3aa7d6ce0fa2": "What are the potential risks associated with membership inference in the context of biometric and personal information?", "afd80b83-937e-4341-a490-62d6c2d66d3e": "How can feedback from end-users and stakeholders be utilized to improve the design of provenance data-tracking techniques?", "5e0fe308-fdea-44d1-a003-3a2896a6d68c": "What are biological design tools mentioned in the context?", "6c4291c9-3495-47ef-b154-fba68e7d27fd": "Where can the document related to biological design tools be accessed?", "980f3b12-24f7-470b-bf59-f44d94116755": "How can organizations maximize the utility of provenance data in their risk management efforts?", "73fff761-237b-4772-84a9-c267a0499e05": "What role does direct input from end users play in enhancing content provenance compared to automated error collection systems?", "8210f7e9-f4b9-4da7-8285-b931cc7dd806": "What are the main predictors of non-consensual dissemination of intimate images according to Karasavva et al (2021)?", "337ced9f-7ecf-47a4-b53f-7d682928c8dd": "How does Khan et al (2024) describe the value chain analysis of generative AI?", "aa5dd42f-b065-46f0-af48-948b025b5c66": "What methodologies are suggested for evaluating potential biases and stereotypes in AI-generated content?", "e8b30601-76c0-4cf0-864c-1a1794332fcf": "How can impact assessments help in understanding the effects of AI-generated content on different social, economic, and cultural groups?", "d019e8b1-2856-420d-b7de-7ef7a8d2961f": "What are the potential harms associated with AI incidents as defined in the context?", "cdcb1002-f726-47be-9c0a-5b9fdb24f0d0": "How can AI incidents manifest in terms of their impact on individuals or groups?", "391f6cc7-d91a-433d-85ae-d32daf4a3825": "What are the main concerns addressed in the paper by Greshake et al (2023) regarding LLM-integrated applications?", "7e302a30-567b-4eca-95bb-0a36c46e63d3": "How does Hagan (2024) propose to establish quality standards for AI responses to legal problems?", "b94c210f-9896-44d9-ae94-75d4fd857d83": "What is the main focus of the paper by Boyarskaya et al (2020) regarding AI system development and deployment?", "ee413cdb-8fcc-4403-afcc-ec9e89c1c9f2": "According to the article by Burgess (2024), what is identified as a significant security flaw in generative AI?", "dcd6f85a-cb22-4829-8ba3-ab5b8b459ade": "What are some purposes that feedback activities can serve in organizations?", "ea734d63-64e6-4e98-bfc9-051508b3e4a3": "What best practices should organizations follow when implementing feedback activities?", "8ad9afc0-623a-4a56-a3f1-3e41513f7694": "What are the potential benefits of combining GAI-led red-teaming with human teams in AI risk assessment?", "0f5bd4ff-9e87-4fc3-819b-84c77a1fdb69": "How might GAI technologies impact the challenges associated with content provenance, particularly in relation to deepfake content?", "91a3ff45-46e4-4c5f-9e59-304220344555": "What are the potential forms of latent systemic bias that can arise in GAI systems due to the characteristics of the training data?", "164e0b09-4266-496a-abe7-397e5523f340": "How might the digital divide affect the representativeness of demographic groups in GAI system training data?", "3784e320-fcfd-48f8-b193-f4ceff3296c4": "What is the title of the paper by Acemoglu published in 2024 regarding AI and macroeconomics?", "64b4e364-3275-4cf2-a94e-f13417a83069": "What topic does the 2024 survey by Atherton focus on in relation to deepfakes?", "b8a4e49b-a5c4-461e-9d77-e85a50193509": "What processes should be established for post-deployment monitoring of GAI systems to address potential risks?", "3b0f1743-1889-420f-ba07-ff5168e60dba": "How can sentiment analysis be utilized to assess user sentiment towards GAI content performance?", "ffed207b-25d2-4c00-b788-39a9181534a4": "What processes should be implemented for real-time monitoring of generated content performance?", "126d7df0-a6e1-4b98-8720-964fbc716784": "How can deviations from desired content standards be identified and addressed?", "6c70a7c5-6b93-4afd-8af3-325d1345b755": "What are the current limitations in formal channels for reporting and documenting AI incidents?", "c398ef46-5582-4ac6-b3cb-ec5febc4cdd8": "How do publicly available databases decide which AI incidents to track?", "3307780d-1bda-4437-ad27-620f33d7fddb": "What are some applications and contexts of use for GAI systems mentioned in the context?", "98cb2689-7822-4970-a0a3-c82fef784ec9": "How can organizations manage the risks associated with AI applications according to the provided context?", "e7c62948-93ad-45d6-a135-622f621e5381": "How can corresponding applications enhance awareness of performance changes in GAI systems?", "22dd1d08-8b0a-4436-ac48-4130b4015c53": "What role does user feedback play in assessing the efficacy and vulnerabilities of GAI systems?", "23e702c3-6e1b-4159-9bcc-534872711452": "What procedures are in place for the review and maintenance of policies regarding newly encountered uses of the GAI system?", "39c4d62c-5f1d-4742-a7c3-557049daef1a": "How are response and recovery plans verified to ensure they include necessary details for communication with downstream GAI system Actors?", "cb72eafa-47f0-469d-aa6b-5ecc2a13ea77": "What methods can be used to assess the general awareness of end users and impacted communities regarding feedback channels for AI systems?", "aa23fdfb-e4d1-4dc9-a38d-590f02af0b66": "How can the trustworthiness of AI systems in deployment contexts be measured and validated according to the provided context?", "e724a9ab-2133-41d8-8fa2-6dd08cc5b187": "What processes are suggested for identifying emergent GAI system risks according to MEASURE 32?", "4494c048-0fb1-43e7-8a61-6a4d2a9f9875": "How are feedback processes for end users and impacted communities integrated into AI system evaluation metrics as outlined in MEASURE 33?", "889d070a-3b0f-484f-8b9d-f3f545db0f4e": "What are the key tasks involved in AI deployment according to the context?", "b33f2b99-0f5c-42a8-8b0b-4a1603873e6d": "Who are the stakeholders mentioned in the context that are involved in AI impact assessment?", "d1f2c4e7-f3f5-4171-b617-d513e7a4aa29": "What are the suggested actions for managing AI risks that do not surpass organizational risk tolerance?", "4858f376-186f-4614-b276-e4bea945f5e9": "How should organizations respond to GAI risks that exceed their risk tolerance according to the provided context?", "7ae846bb-0126-4be0-96bf-71de20a8c1df": "What are the key components involved in the evaluation of privacy risks in AI systems as outlined in the context?", "5b7552db-d258-4a83-bc09-db9dc72012a8": "How does AI red-teaming contribute to identifying potential risks associated with model extraction and training data output?", "60cecc65-0cd9-4d5c-a2ca-31027da15891": "What are the key capabilities and limitations of monitoring systems in the deployment of GAI technologies?", "36052c3a-89fc-4bbf-976c-5d4eeacbb99c": "How can organizations effectively document GAI system objectives to identify gaps in provenance data usage?", "6e09ae0c-ecb4-4c87-aa60-f578f0755382": "What are the short, mid, and long-term impacts of AI in cybersecurity as discussed by De Angelo (2024)?", "857f3171-121e-4cce-b4b1-bd72497f9ace": "How do chatbots relate to mental health and the safety of generative AI according to De Freitas et al (2023)?", "e24a9fcb-7f30-4b81-8f46-ead077aa079e": "What are the main themes discussed in Chandra et al (2023) regarding Chinese influence operations?", "fc491105-1f0d-4881-9572-4db26cee7932": "How do Ciriello et al (2024) address the ethical tensions associated with human-AI companionship in their inquiry into Replika?", "9b74ae8b-9d3a-49be-aa7e-b819633cae4b": "What is the main focus of Tirrell's (2017) work on toxic speech and discursive harm?", "ae200e69-3bf3-4167-b9f4-348604a9c9a9": "How do Tufekci's (2015) findings address the challenges of computational agency beyond major tech platforms?", "c2082a9e-1c32-4d4b-a21d-7d08582ee55f": "What is the focus of the National Institute of Standards and Technology's 2022 publication regarding artificial intelligence?", "02955841-900b-43b0-9603-24e168d955b4": "How do pervasive label errors in test sets affect machine learning benchmarks according to Northcutt et al (2021)?", "2d73c410-fffa-46db-96c3-b6b2adbbb39d": "What aspects of the training process for the pre-trained model are highlighted in the context, including hyperparameters and fine-tuning methods?", "a0ed36d8-1343-428e-9845-c1031d400a56": "How does the system evaluate user-reported problematic content and integrate feedback into updates?", "de5105db-27c4-4545-b4fc-6ccf2c684c01": "What is the focus of the article from the Canadian Centre for Cyber Security regarding generative artificial intelligence?", "6f39bf39-1e6f-415f-8bc4-23da096c9e10": "What are the main findings discussed in the paper by Carlini et al (2023) on quantifying memorization across neural language models?", "07f0015c-a710-4525-b1b8-ea57276f00bb": "What is the purpose of creating measurement error models for pre-deployment metrics in the context of TEVV?", "86f7a3bf-e33f-43f4-a3b5-4553c1f53ef4": "How can domain expertise be utilized when modeling complex societal constructs such as hateful content?", "b81923a6-0ccd-44af-be75-85d995b3e069": "What mechanisms are suggested to sustain the value of deployed AI systems according to the context provided?", "9d49482b-6f4b-4912-9e9d-cf0dbf5f888b": "How should GAI system outputs be evaluated in relation to organizational risk tolerance and guidelines?", "0c2fee66-cf87-42e9-b12c-6558eb427e32": "What are the main themes discussed in Duhigg's article about how companies learn consumer secrets?", "7fbb3695-3951-4f64-8bd6-dbd5cb229016": "How do the findings in Elsayed et al's research relate to the influence of altered images on human perception?", "0e09cbf8-4f80-45a3-b83d-b0d259f8b20f": "What are the main themes discussed in the paper \"Algorithmic Pluralism: A Structural Approach To Equal Opportunity\" by Jain et al (2023)?", "30dec49c-94c5-4675-87e6-95b392d5ed54": "How do the findings in the study by Jones-Jang et al (2022) relate to public perceptions of AI failure?", "db75d6e9-08eb-480b-b041-306f149025ce": "How can documenting and reporting GAI incidents assist AI Actors in managing risks?", "1e369a9d-581d-44a8-aa02-773e8baf287f": "What measures can organizations implement to promote transparency in GAI incident reporting?", "601b142e-a812-41de-a66e-073a12ed50bc": "What are some examples of structured public feedback methods mentioned in the context?", "481dc8aa-cd5c-4c49-859f-57c200a6292b": "How might gaps between benchmarks and real-world use of GAI systems be affected by prompt sensitivity?", "8299dd11-928e-4e65-b0ba-18f0c273a386": "What are the key responsibilities of AI system operators in identifying GAI incidents across the AI lifecycle?", "d135471b-4bbf-4b4b-8686-226c120e0894": "Why is documentation and review of third-party inputs and plugins important for AI Actors in the context of incident disclosure?", "548532d2-fac1-4858-84eb-117f275eebe7": "What role does provenance data tracking play in distinguishing between human-generated and AI-generated content?", "15d0b225-9ed3-4549-9445-ff6c39cc67a9": "How can digital transparency mechanisms improve public trust in AI systems?", "c50db019-f4ed-468f-ac4e-cdff518ac43c": "What are the responsibilities of AI Actors in evaluating GAI system performance and addressing reported issues?", "3726f527-d18e-4a38-a9f4-15649630c69b": "How are measurable activities for continual improvements integrated into AI system updates according to the context provided?", "16d6e520-d6e3-427f-a506-b572e1406542": "What methods are suggested for quantifying harms related to generated content exhibiting harmful bias?", "88085724-ce49-4511-ae3a-2eeb8c1771a5": "Which general fairness metrics are recommended for evaluating ML pipelines or business processes that utilize GAI?", "c14fbc2a-fbe4-4262-ba7c-ff9d8082f720": "What are the key risks associated with the value chain of GAI systems that need to be tested?", "52e1488b-ebce-49ab-835b-2a0ca6403be0": "How should organizations reassess risk measurements after fine-tuning third-party GAI models?", "eb59e967-95a8-4741-91f5-889ccae14849": "What is the importance of domain expertise and socio-cultural awareness in AI red-teaming?", "cd822aa7-19d4-41f2-9b6a-c7d76073bd50": "How should AI red-teaming results be analyzed before being integrated into organizational governance?", "3be7b9a4-8eae-45ad-a9c4-5ea4a19bead1": "What are some key practices mentioned for ensuring data protection and retention in the context of GAI use?", "a52fbc58-8a61-4e0e-8492-385a4e68a0be": "How can establishing acceptable use policies for GAI in human-AI teaming settings help mitigate risks?", "28b7cd21-e33f-41da-9dc9-2fd6d48792f5": "What processes are followed and documented for tracking, responding to, and recovering from incidents and errors in AI systems?", "4e82f066-d3de-4b27-8970-e946f82491bd": "How are incidents communicated to relevant AI Actors and affected communities according to the suggested actions for GAI system incidents?", "5736a975-e631-44fa-93af-56d16590f176": "What are the primary considerations derived from the GAI PWG consultation process?", "98e901c6-af83-4598-9cb8-e73066d32174": "Who contributed to the analysis and development of the primary considerations for GAI?", "e95aa42e-102d-48ca-814b-d111aa07c7ea": "What is the primary purpose of AI red-teaming in the context of AI model development?", "18b9b79c-a63e-46da-b922-adbb30c6e3ce": "How does the diversity of an AI red team impact the quality of its outputs?", "9e890f45-38c6-4d29-a9a8-9d3ebbd79138": "What is the focus of the National Institute of Standards and Technology's AI Risk Management Framework as outlined in Appendix B?", "6e9e7b6e-f97e-456f-b257-6a48cfc3995e": "Where can one find the AI RMF Playbook published by the National Institute of Standards and Technology?", "ddd80e62-a4ee-4d6c-b3c8-e5b5bfb56cba": "What is the focus of the paper by Padmakumar et al (2024) regarding language models and content diversity?", "eefba1f8-d376-4c55-90bc-b7161ed97040": "What are the main topics covered in the survey by Park et al (2024) on AI deception?", "019a1127-ded5-40f9-92e7-7fa9ed08865a": "What is the purpose of the 15 Winogender Schemas in natural language processing?", "4d79174e-e62b-405a-b787-f85ff98ca12c": "How do the 15 Winogender Schemas differ from each other?", "f57fbda5-4575-4006-8809-9f9a29b25f41": "What techniques can be employed to mitigate representational biases in AI-generated content?", "d2c8cd63-bda6-4922-bf9a-b00d90471302": "How can real-time monitoring systems ensure the effectiveness of content provenance protocols in GAI systems?", "f8b5fdbc-011e-4241-8401-36d60af31268": "What role do representative AI Actors play in the lifecycle mentioned in the context?", "914f02e3-0530-465c-a447-2d5d69efbcd9": "What is the significance of Figure 3 in relation to the AI RMF?", "52b06058-b864-4868-82a4-129ad6fe6575": "What processes are suggested to mitigate risks associated with unexplainable GAI systems?", "c5dc95e4-4469-4efc-bc88-8c6b570ffc68": "How should the adaptation of pre-trained models for specific generative tasks be documented?", "bab49c47-17ee-40b0-88f2-04d4d139159b": "What benchmarks are suggested for quantifying systemic bias in GAI system outputs according to Measure 211?", "737ad1d7-1dba-40cf-a284-4cfa21e61c54": "What should be documented regarding the assumptions and limitations of the benchmarks used in the fairness assessments?", "593779c4-57d0-409b-931a-7a87103e048a": "What are the main themes discussed in the paper \"Algorithmic monoculture and social welfare\" by Kleinberg et al?", "6bfaea2b-8756-453b-8110-9e5eeaeef4d3": "How do the findings in \"GPT detectors are biased against non-native English writers\" by Liang et al contribute to the understanding of AI biases?", "aaf9e9e2-0243-498f-8ed2-14dd7f71da6f": "What is the main focus of the paper by Kalai, A, et al (2024) titled \"Calibrated Language Models Must Hallucinate\"?", "9daa5424-fce3-4699-af81-45dd9698f66f": "Where can the paper by Kalai, A, et al (2024) be accessed?", "3574c243-2ba7-4e30-90b6-469375f2ef08": "What policies and procedures should be established to record and track GAI system reported errors?", "097bc932-96a7-45c0-8d66-995d4707d795": "How can organizations ensure the integrity of information when tracking near-misses and negative impacts?", "7545e3df-f582-46eb-bbe9-d91a7f7e3841": "What are the anticipated environmental impacts that should be documented during model development, maintenance, and deployment in product design decisions?", "18df301d-2e0b-4691-90d6-afb1f7887590": "How can organizations measure or estimate the environmental impacts associated with training, fine-tuning, and deploying models?", "1b249785-7927-4fcb-a080-f167c87640f4": "What are the potential risks associated with third-party GAI integrations mentioned in the context?", "40f4fb84-d66e-4687-981f-4de2d0935e5b": "How can organizations manage the risks related to the collection and use of third-party data for model inputs?", "8ed16b02-cadb-40b6-87bc-91cf8e67e619": "What steps should be taken to adapt processes based on findings from incidents involving inappropriate or harmful content?", "ec20002c-336a-4325-a842-68608d653bc8": "How can visualizations be utilized to help non-technical stakeholders understand the functionality of GAI systems?", "4f73df4b-a82b-4b6e-973f-a010939ab192": "What mechanisms are included in the post-deployment AI system monitoring plans for evaluating input from users and other relevant AI Actors?", "bed948fa-cec2-4cf8-8e2e-bc35ae3c92e6": "How can collaboration with external researchers and industry experts help in managing identified risks associated with AI systems?", "9d70d6c8-a54b-4d21-a106-738158b93096": "What are the main concerns addressed in the paper by Qu, Y et al (2023) regarding text-to-image models?", "f128e39f-e4b9-42ab-bd26-59c1933717cb": "How does the study by Rafat, K et al (2023) propose to mitigate the carbon footprint associated with deep learning model compression?", "013b572a-70bc-4dcf-bfd2-c42f64c0b94c": "What are the key characteristics of AI risks and trustworthiness as outlined in Chapter 3 of the AI Risk Management Framework by the National Institute of Standards and Technology?", "19ba9289-301c-4891-ad40-f04a912fa0f1": "How does Chapter 6 of the AI Risk Management Framework define AI RMF profiles and their significance in managing AI risks?", "eaaeca4c-30ba-4428-b9de-751a68d29842": "What are the key components involved in third-party transparency and risk management for GAI systems?", "76133783-4f40-4a8c-b7f6-b52d56417b29": "How do robust test, evaluation, validation, and verification (TEVV) processes contribute to pre-deployment measurement efforts for GAI systems?", "5896ea53-8ec9-4e22-b1c8-c57ffc17935d": "What are the key considerations for organizations when applying governance principles to generative AI models?", "0e6fe724-4a0e-491d-a4c4-8e92db055aa7": "How might organizations choose to revise their existing risk tiering for generative AI systems?", "242ba570-059a-41bc-9754-7b2aeb1feef6": "What are some potential issues with using intelligence tests and professional licensing exams to assess GAI system validity and reliability?", "66864198-dc8e-4fb5-a8ed-4c05784a7920": "How do measurement gaps between laboratory settings and real-world conditions affect the assessment of GAI impacts?", "58641177-1bd3-4f04-bda0-34e7909d87c9": "What are the main concerns discussed in Satariano et al (2023) regarding the impact of deepfake technology?", "a8e758c8-3ffb-47f6-8ccf-e5426c17a708": "How do Schaul et al (2024) describe the role of certain websites in shaping the intelligence of AI like ChatGPT?", "e1befd8b-395e-49c4-9a9f-50a07faadeed": "What are the ethical and social risks associated with language models as discussed by Weidinger et al (2021)?", "45ed54ce-244f-452a-9049-b0fd17680302": "How does the research by West (2023) highlight the risks that AI poses specifically to women?", "5461812d-52b2-496f-8d51-f04c3ffdece9": "What are some of the key differences in oversight requirements between generative AI (GAI) and non-generative AI tools?", "2f846983-e1f7-43ed-96b1-749d41e2075f": "How might the varied outputs and user interfaces of AI technology influence the interaction of different AI Actors with GAI systems?", "b3e69079-0b01-4e5c-9099-3559b496c609": "What are some examples of transparency artifacts that should be reviewed for third-party models?", "48e21639-e6ca-4fe0-b29f-677fcbaae9b9": "How are pre-trained models monitored as part of the regular maintenance of an AI system?", "045e779f-6e28-46ee-8fdf-9d59ae52dc4d": "What methods can be implemented to evaluate the decisions made by GAI systems for interpretability and explainability?", "4d938d3c-06d3-47ac-91ad-464d3ac6b987": "How can vulnerabilities and potential misuse scenarios of GAI systems be identified and addressed?", "1ab8b7f7-eef4-4e07-bb3d-b60bc52672f8": "What procedures are suggested for responding to and recovering from previously unknown risks in GAI development?", "1583d955-37bd-4f9a-bc84-21545c7b0542": "How can GAI development enhance techniques while ensuring data privacy and preventing harmful bias?", "2644f30a-d5d2-41fb-ad4d-28581c9619d6": "What methods can be employed to assess the impact of AI-generated content based on feedback from various stakeholders?", "a591ab4c-66f4-4fb4-8396-130e566684d5": "How can real-time auditing tools contribute to the validation of the lineage and authenticity of AI-generated data?", "213b0a20-1b3f-4e97-a949-46a9f2c52ade": "What are the main concerns discussed in Luccioni et al (2023) regarding the cost of AI deployment?", "5a4b96fe-9564-4c4d-b3a6-e2ac6e309187": "How does the research by Mouton et al (2024) address the operational risks associated with AI in the context of large-scale biological attacks?", "0b2d0a39-0eb1-4965-bc28-7bd4638fc7a9": "What measures are suggested to assess the proportion of synthetic to non-synthetic training data in AI model training?", "db6156e9-b366-4fcf-b4db-7cdba7467e4b": "How should the environmental impact and sustainability of AI model training and management activities be documented according to the provided context?", "c1b82b54-4de6-472d-b782-a3d6aae5981c": "What are the main concerns addressed in the paper by Staab et al (2023) regarding large language models and privacy?", "1be63ff2-e18a-4f5d-a39d-7b3004f4f0b0": "How do Stanford et al (2023) investigate the representation of opinions in language models?", "a765dc09-b0b8-4d82-8528-4ff7293d5696": "What are the potential risks associated with providing instructions and material that may elicit harmful model behaviors in AI red-teaming exercises?", "8768f209-ad73-419c-8bdc-9aa3b25cbef0": "How can AI red-teaming exercises benefit from the involvement of large groups of participants?", "9dfafa5f-851e-4f62-b067-a4a694440876": "What are the legal and regulatory requirements for reporting GAI incidents mentioned in the context?", "7d8a481f-60df-4642-b581-e9eca48751e7": "Which organizations' reporting requirements are referenced in relation to HIPAA and autonomous vehicle crashes?", "54781ebf-46f8-4a96-a116-af7afca2dbf7": "What methods are suggested for measuring the prevalence of denigration in generated content during deployment?", "f68bba8d-981e-47c0-9870-1bc0e9d7d1a2": "How can direct engagement with potentially impacted communities help identify the classes of individuals or groups affected by GAI systems?", "51906f31-37bd-40ec-b89e-d078936fc2e9": "What role do organizational boards or committees play in the deployment of GAI applications and content provenance when using third-party pre-trained models?", "45cf1ea4-8333-4461-b4c3-2b2927de0941": "How should human moderation systems be implemented in relation to human-AI configuration policies and socio-cultural norms?", "25aa3569-561c-407e-a326-c008e252278b": "How can the selection of a watermarking model impact computational complexity in AI systems?", "826d4c22-9e20-4395-9fec-717374113106": "What are some organizational risk management efforts for enhancing content provenance in GAI systems?", "a78d9d4b-b448-41c1-b918-8545493d5635": "What are some examples of participatory engagement methods that organizations can use to involve external stakeholders in product development?", "9d1e4a84-8573-45f0-b27e-09a3b89447f3": "How does field testing differ from participatory engagement methods in terms of structure and purpose?", "2d333e53-5026-4784-bae3-10ff62a59ac5": "What steps should be included in the communication plans for deactivating a GAI system?", "5c34e7ae-6641-4858-8c02-5ed37a2e572c": "What are the potential risks associated with the performance of AI systems that may lead to their deactivation?", "1b208753-5ed0-4b6b-b5f7-7cef5f7fd913": "What are the main findings of the study by Wang, X et al (2023) regarding energy and carbon considerations in fine-tuning BERT?", "fb301fda-e339-48af-b835-3d98eef5d599": "How does the dataset \"Do-Not-Answer\" contribute to the evaluation of safeguards in large language models according to Wang, Y et al (2023)?", "79c76166-16c1-41da-bd19-c6cc8c7ac7c2": "What are the capabilities and limitations of GAI systems in relation to digital content transparency for AI actors and the public?", "e4822dae-129e-46bb-a2e9-577b9b03cb3a": "How can structured feedback about content provenance be effectively recorded and integrated from various stakeholders, including operators and impacted communities?", "375e3521-b26f-48bc-a4b2-4aeef612b534": "What are the key considerations for organizations when conducting field style tests on GAI systems?", "96488a38-0605-447d-a314-8e1fb1d648b1": "How can organizations ensure compliance with human subject standards when collecting user feedback on AI systems?", "f0995299-9d06-4ac9-ad49-4edd6d3840c2": "What are the key priorities outlined in the White House's 2022 Roadmap for Researchers regarding information integrity?", "7520279f-810e-4e74-8e50-658e9838b2fd": "What findings were reported by the Stanford Cyber Policy Center regarding AI image generation models and their training data?", "be59a702-b803-47aa-ae1f-c99589cba877": "What steps should be taken to monitor and document instances where human operators override the GAI's decisions?", "1a6e587c-2ac1-486b-8627-860e1d0c3355": "How can the results of structured public feedback exercises be verified and incorporated into the AI deployment process?", "8f85608d-6b0b-4c32-9daf-49a64954aa05": "What steps should be taken to enhance transparency and accountability in the GAI system according to MG-41-005?", "2939cc15-d13f-4217-9731-4e9a0f7b5896": "How can organizations track dataset modifications to ensure the provenance of data as outlined in MG-41-006?", "b24de514-9dc6-46eb-b84d-07fe10961f08": "What measures should be taken to review training data for CBRN information and intellectual property?", "103f3a45-fc7b-4c73-b9a8-1bea99f57b5b": "How can organizations prevent or respond to outputs that reproduce particular training data?"}, "relevant_contexts": {"2f747c80-2d00-4016-85a8-a0c1e76b3703": ["981b4a03-c6d6-469b-8883-80136cc9bb75"], "f869267f-945d-41e3-b342-2304e04c057d": ["981b4a03-c6d6-469b-8883-80136cc9bb75"], "c2ef8666-75a2-42b1-9d3a-e8e418efcec1": ["0d8dcce6-ac7c-4956-ad10-88d8d289a7bc"], "e45f9562-6623-4a00-be37-9b9d5fbfbb69": ["0d8dcce6-ac7c-4956-ad10-88d8d289a7bc"], "76919887-444a-4957-9eea-e4889495d3f4": ["ac8df4fa-5e36-4a73-a5c9-6689d9ac38b0"], "f87cfeb7-1602-4d60-be72-057e8e1bda33": ["ac8df4fa-5e36-4a73-a5c9-6689d9ac38b0"], "f09a8cdc-9835-49f6-9172-a7f0a0d38240": ["7baa9ec7-bcd0-4f77-9325-e1e8bd8b2c7e"], "fc5d5742-0f43-4817-9de4-45be387d06be": ["7baa9ec7-bcd0-4f77-9325-e1e8bd8b2c7e"], "11b8d4bf-83e1-426c-8ef4-e6ef4fd271f8": ["def3fa26-1ab7-4e71-8e89-1ae0d62bfbd4"], "da4ccead-3cfc-4260-9b68-e4f492505a25": ["def3fa26-1ab7-4e71-8e89-1ae0d62bfbd4"], "d364fd5d-6cdd-486d-b55b-35a2ce00dca5": ["823c91d4-2c65-4788-82a4-a87a98cf5301"], "0599add6-1dc0-4741-9520-3dc8f324292f": ["823c91d4-2c65-4788-82a4-a87a98cf5301"], "a531e7c4-5222-4f61-9346-0d8717f462fb": ["b8e57007-cb42-451b-b815-c73eb5b0146b"], "504213b1-86dc-402c-88e9-ecbc4c62dddf": ["b8e57007-cb42-451b-b815-c73eb5b0146b"], "63023e75-06fc-4be6-8964-fe16f89ae3ef": ["c0627672-0f3d-4a79-9b0e-7fd49ff9979d"], "375c72a7-651d-4c66-8c79-03278dee8c86": ["c0627672-0f3d-4a79-9b0e-7fd49ff9979d"], "f363b703-0ff8-4ba2-ae8b-3e7a2d511c47": ["f4f6180f-f191-4f4a-a27b-6173b0cb7f56"], "3e96af30-e33a-49f5-b066-1346e4d80b97": ["f4f6180f-f191-4f4a-a27b-6173b0cb7f56"], "ed60263f-eabd-42bb-a5fa-c2f61e88c0a0": ["c26e6566-e2d6-41d7-947f-4082567b1645"], "407e0915-d7f2-4ac1-a348-0ee545587238": ["c26e6566-e2d6-41d7-947f-4082567b1645"], "a39abfd0-b757-4be0-b3aa-261af691ac23": ["076b15b9-9812-4e46-b701-f0e3a5eb92ce"], "077279e1-81ca-42dc-8f4d-a87e9a2aed9d": ["076b15b9-9812-4e46-b701-f0e3a5eb92ce"], "55aa2480-8133-459a-9df1-3aa7d6ce0fa2": ["13040126-23d8-4d11-a7fc-6c90cf61c745"], "afd80b83-937e-4341-a490-62d6c2d66d3e": ["13040126-23d8-4d11-a7fc-6c90cf61c745"], "5e0fe308-fdea-44d1-a003-3a2896a6d68c": ["cc45443a-58a2-4592-bf3c-8d5243666ed2"], "6c4291c9-3495-47ef-b154-fba68e7d27fd": ["cc45443a-58a2-4592-bf3c-8d5243666ed2"], "980f3b12-24f7-470b-bf59-f44d94116755": ["94f85952-e349-4b21-a566-fa13067c0574"], "73fff761-237b-4772-84a9-c267a0499e05": ["94f85952-e349-4b21-a566-fa13067c0574"], "8210f7e9-f4b9-4da7-8285-b931cc7dd806": ["715a5a00-13e4-4b9d-ac06-318c21f3a293"], "337ced9f-7ecf-47a4-b53f-7d682928c8dd": ["715a5a00-13e4-4b9d-ac06-318c21f3a293"], "aa5dd42f-b065-46f0-af48-948b025b5c66": ["444359ee-b99a-42bc-bf97-c2938e4caa18"], "e8b30601-76c0-4cf0-864c-1a1794332fcf": ["444359ee-b99a-42bc-bf97-c2938e4caa18"], "d019e8b1-2856-420d-b7de-7ef7a8d2961f": ["d46635b7-6805-4b6b-a2f9-7ad9d71372b9"], "cdcb1002-f726-47be-9c0a-5b9fdb24f0d0": ["d46635b7-6805-4b6b-a2f9-7ad9d71372b9"], "391f6cc7-d91a-433d-85ae-d32daf4a3825": ["ade5dcda-567c-41ab-ad1c-38f6e12ec243"], "7e302a30-567b-4eca-95bb-0a36c46e63d3": ["ade5dcda-567c-41ab-ad1c-38f6e12ec243"], "b94c210f-9896-44d9-ae94-75d4fd857d83": ["f7603c35-fcfb-4c42-bf03-1c14db589026"], "ee413cdb-8fcc-4403-afcc-ec9e89c1c9f2": ["f7603c35-fcfb-4c42-bf03-1c14db589026"], "dcd6f85a-cb22-4829-8ba3-ab5b8b459ade": ["ca324670-5ba1-448d-a546-4c66c8e5059b"], "ea734d63-64e6-4e98-bfc9-051508b3e4a3": ["ca324670-5ba1-448d-a546-4c66c8e5059b"], "8ad9afc0-623a-4a56-a3f1-3e41513f7694": ["e59ff7fc-cb53-49b7-b47e-67feebf7da9b"], "0f5bd4ff-9e87-4fc3-819b-84c77a1fdb69": ["e59ff7fc-cb53-49b7-b47e-67feebf7da9b"], "91a3ff45-46e4-4c5f-9e59-304220344555": ["13f2b508-0ffa-44b4-b495-d0f192adce01"], "164e0b09-4266-496a-abe7-397e5523f340": ["13f2b508-0ffa-44b4-b495-d0f192adce01"], "3784e320-fcfd-48f8-b193-f4ceff3296c4": ["87c3bc49-05ad-49ac-8666-1d19006bf268"], "64b4e364-3275-4cf2-a94e-f13417a83069": ["87c3bc49-05ad-49ac-8666-1d19006bf268"], "b8a4e49b-a5c4-461e-9d77-e85a50193509": ["c0aeda94-1d7e-431f-b056-2b04f9bdb2b6"], "3b0f1743-1889-420f-ba07-ff5168e60dba": ["c0aeda94-1d7e-431f-b056-2b04f9bdb2b6"], "ffed207b-25d2-4c00-b788-39a9181534a4": ["95ea404a-34d5-4183-b7cf-518c69f14804"], "126d7df0-a6e1-4b98-8720-964fbc716784": ["95ea404a-34d5-4183-b7cf-518c69f14804"], "6c70a7c5-6b93-4afd-8af3-325d1345b755": ["9754ca48-303f-4cb1-b99e-37d07fd12ad3"], "c398ef46-5582-4ac6-b3cb-ec5febc4cdd8": ["9754ca48-303f-4cb1-b99e-37d07fd12ad3"], "3307780d-1bda-4437-ad27-620f33d7fddb": ["608bd561-c17d-42a6-a801-7c117c715fd3"], "98cb2689-7822-4970-a0a3-c82fef784ec9": ["608bd561-c17d-42a6-a801-7c117c715fd3"], "e7c62948-93ad-45d6-a135-622f621e5381": ["31f7f2aa-6a43-48c9-bc75-d9df3fc14a32"], "22dd1d08-8b0a-4436-ac48-4130b4015c53": ["31f7f2aa-6a43-48c9-bc75-d9df3fc14a32"], "23e702c3-6e1b-4159-9bcc-534872711452": ["370ddbe4-2469-44c9-af19-34b01ffd2689"], "39c4d62c-5f1d-4742-a7c3-557049daef1a": ["370ddbe4-2469-44c9-af19-34b01ffd2689"], "cb72eafa-47f0-469d-aa6b-5ecc2a13ea77": ["b4abb967-84d6-4153-9be8-189e17710295"], "aa23fdfb-e4d1-4dc9-a38d-590f02af0b66": ["b4abb967-84d6-4153-9be8-189e17710295"], "e724a9ab-2133-41d8-8fa2-6dd08cc5b187": ["6a2894a1-7754-43ab-84d6-838cf40df894"], "4494c048-0fb1-43e7-8a61-6a4d2a9f9875": ["6a2894a1-7754-43ab-84d6-838cf40df894"], "889d070a-3b0f-484f-8b9d-f3f545db0f4e": ["32009780-7da5-46ae-b978-9e75ead5ab84"], "b33f2b99-0f5c-42a8-8b0b-4a1603873e6d": ["32009780-7da5-46ae-b978-9e75ead5ab84"], "d1f2c4e7-f3f5-4171-b617-d513e7a4aa29": ["71a468cc-1791-45cc-b518-84b7f6a84fba"], "4858f376-186f-4614-b276-e4bea945f5e9": ["71a468cc-1791-45cc-b518-84b7f6a84fba"], "7ae846bb-0126-4be0-96bf-71de20a8c1df": ["64a2f22c-c3ca-43fb-9f8c-450dbe50a276"], "5b7552db-d258-4a83-bc09-db9dc72012a8": ["64a2f22c-c3ca-43fb-9f8c-450dbe50a276"], "60cecc65-0cd9-4d5c-a2ca-31027da15891": ["318ef638-2789-4e83-83f7-5a5786dab2cb"], "36052c3a-89fc-4bbf-976c-5d4eeacbb99c": ["318ef638-2789-4e83-83f7-5a5786dab2cb"], "6e09ae0c-ecb4-4c87-aa60-f578f0755382": ["3cfdfc37-c2be-4e2e-8336-566a2b07c878"], "857f3171-121e-4cce-b4b1-bd72497f9ace": ["3cfdfc37-c2be-4e2e-8336-566a2b07c878"], "e24a9fcb-7f30-4b81-8f46-ead077aa079e": ["876196aa-616e-4c86-9a95-15b8b5e9aa2f"], "fc491105-1f0d-4881-9572-4db26cee7932": ["876196aa-616e-4c86-9a95-15b8b5e9aa2f"], "9b74ae8b-9d3a-49be-aa7e-b819633cae4b": ["155eb125-4e17-4ad2-93e5-3e8313f40a5c"], "ae200e69-3bf3-4167-b9f4-348604a9c9a9": ["155eb125-4e17-4ad2-93e5-3e8313f40a5c"], "c2082a9e-1c32-4d4b-a21d-7d08582ee55f": ["5a43d2ed-3c98-4a72-b479-a4883aa41fd3"], "02955841-900b-43b0-9603-24e168d955b4": ["5a43d2ed-3c98-4a72-b479-a4883aa41fd3"], "2d73c410-fffa-46db-96c3-b6b2adbbb39d": ["f7e95a75-2281-4b96-9b12-c9c0a971bc9f"], "a0ed36d8-1343-428e-9845-c1031d400a56": ["f7e95a75-2281-4b96-9b12-c9c0a971bc9f"], "de5105db-27c4-4545-b4fc-6ccf2c684c01": ["82ddc0ca-3af2-46ec-bbc9-f0bd3a3c13f1"], "6f39bf39-1e6f-415f-8bc4-23da096c9e10": ["82ddc0ca-3af2-46ec-bbc9-f0bd3a3c13f1"], "07f0015c-a710-4525-b1b8-ea57276f00bb": ["6522b45d-8703-4d38-9df0-18ddaae3c7fb"], "86f7a3bf-e33f-43f4-a3b5-4553c1f53ef4": ["6522b45d-8703-4d38-9df0-18ddaae3c7fb"], "b81923a6-0ccd-44af-be75-85d995b3e069": ["a33767a8-d0c1-45c9-a9d5-dd3b24d2ed2b"], "9d49482b-6f4b-4912-9e9d-cf0dbf5f888b": ["a33767a8-d0c1-45c9-a9d5-dd3b24d2ed2b"], "0c2fee66-cf87-42e9-b12c-6558eb427e32": ["833eab77-92a5-4d34-8641-0761440d6bef"], "7fbb3695-3951-4f64-8bd6-dbd5cb229016": ["833eab77-92a5-4d34-8641-0761440d6bef"], "0e09cbf8-4f80-45a3-b83d-b0d259f8b20f": ["bc068d9a-3dfe-4ff8-a7a3-c05c0b2eb25d"], "30dec49c-94c5-4675-87e6-95b392d5ed54": ["bc068d9a-3dfe-4ff8-a7a3-c05c0b2eb25d"], "db75d6e9-08eb-480b-b041-306f149025ce": ["28c62c74-a723-402d-bd1c-a3956f871bd1"], "1e369a9d-581d-44a8-aa02-773e8baf287f": ["28c62c74-a723-402d-bd1c-a3956f871bd1"], "601b142e-a812-41de-a66e-073a12ed50bc": ["0e980c31-ca5a-4756-9c7c-768c2d4301c3"], "481dc8aa-cd5c-4c49-859f-57c200a6292b": ["0e980c31-ca5a-4756-9c7c-768c2d4301c3"], "8299dd11-928e-4e65-b0ba-18f0c273a386": ["2562aca8-faed-4716-b0b8-4507aacbea73"], "d135471b-4bbf-4b4b-8686-226c120e0894": ["2562aca8-faed-4716-b0b8-4507aacbea73"], "548532d2-fac1-4858-84eb-117f275eebe7": ["ff012b42-7f94-4bf9-afaf-df4788daa2f5"], "15d0b225-9ed3-4549-9445-ff6c39cc67a9": ["ff012b42-7f94-4bf9-afaf-df4788daa2f5"], "c50db019-f4ed-468f-ac4e-cdff518ac43c": ["10742ac9-7c29-40a8-8418-6986b28192f1"], "3726f527-d18e-4a38-a9f4-15649630c69b": ["10742ac9-7c29-40a8-8418-6986b28192f1"], "16d6e520-d6e3-427f-a506-b572e1406542": ["5bda8728-1192-405c-a66c-dc2245e2858a"], "88085724-ce49-4511-ae3a-2eeb8c1771a5": ["5bda8728-1192-405c-a66c-dc2245e2858a"], "c14fbc2a-fbe4-4262-ba7c-ff9d8082f720": ["8f911958-0105-4b8b-889f-fbdfc27c7195"], "52e1488b-ebce-49ab-835b-2a0ca6403be0": ["8f911958-0105-4b8b-889f-fbdfc27c7195"], "eb59e967-95a8-4741-91f5-889ccae14849": ["dcbd62ea-5c06-440e-8379-608b0a41acd9"], "cd822aa7-19d4-41f2-9b6a-c7d76073bd50": ["dcbd62ea-5c06-440e-8379-608b0a41acd9"], "3be7b9a4-8eae-45ad-a9c4-5ea4a19bead1": ["a8f97317-1e5c-4d8d-9ca3-0b4a547e3946"], "a52fbc58-8a61-4e0e-8492-385a4e68a0be": ["a8f97317-1e5c-4d8d-9ca3-0b4a547e3946"], "28b7cd21-e33f-41da-9dc9-2fd6d48792f5": ["777a10bb-f608-4bbb-af08-7d03a64075ee"], "4e82f066-d3de-4b27-8970-e946f82491bd": ["777a10bb-f608-4bbb-af08-7d03a64075ee"], "5736a975-e631-44fa-93af-56d16590f176": ["fadfc9bd-69c4-46bd-ac7c-6980c5610cdf"], "98e901c6-af83-4598-9cb8-e73066d32174": ["fadfc9bd-69c4-46bd-ac7c-6980c5610cdf"], "e95aa42e-102d-48ca-814b-d111aa07c7ea": ["cf102159-229e-44c7-a1f3-079e8a4db54a"], "18b9b79c-a63e-46da-b922-adbb30c6e3ce": ["cf102159-229e-44c7-a1f3-079e8a4db54a"], "9e890f45-38c6-4d29-a9a8-9d3ebbd79138": ["fe46bbca-da69-42c4-bd66-20a39c3a69a8"], "6e9e7b6e-f97e-456f-b257-6a48cfc3995e": ["fe46bbca-da69-42c4-bd66-20a39c3a69a8"], "ddd80e62-a4ee-4d6c-b3c8-e5b5bfb56cba": ["c65a398d-00ae-4acf-8fb4-31377547e3b5"], "eefba1f8-d376-4c55-90bc-b7161ed97040": ["c65a398d-00ae-4acf-8fb4-31377547e3b5"], "019a1127-ded5-40f9-92e7-7fa9ed08865a": ["068d87d8-51a4-4adc-ac91-23e6f0c5fbcd"], "4d79174e-e62b-405a-b787-f85ff98ca12c": ["068d87d8-51a4-4adc-ac91-23e6f0c5fbcd"], "f57fbda5-4575-4006-8809-9f9a29b25f41": ["16809d4b-78ad-4b8f-8dd2-e0427a5be4f9"], "d2c8cd63-bda6-4922-bf9a-b00d90471302": ["16809d4b-78ad-4b8f-8dd2-e0427a5be4f9"], "f8b5fdbc-011e-4241-8401-36d60af31268": ["4dcc6606-53cf-4f6c-a102-9652fe5197af"], "914f02e3-0530-465c-a447-2d5d69efbcd9": ["4dcc6606-53cf-4f6c-a102-9652fe5197af"], "52b06058-b864-4868-82a4-129ad6fe6575": ["f94103f0-1896-4b0f-a4bd-bbe9a2e5f610"], "c5dc95e4-4469-4efc-bc88-8c6b570ffc68": ["f94103f0-1896-4b0f-a4bd-bbe9a2e5f610"], "bab49c47-17ee-40b0-88f2-04d4d139159b": ["16f69c2f-c82a-41ba-83f7-d15a332f0237"], "737ad1d7-1dba-40cf-a284-4cfa21e61c54": ["16f69c2f-c82a-41ba-83f7-d15a332f0237"], "593779c4-57d0-409b-931a-7a87103e048a": ["551c7cae-a6c1-4903-a574-9aa068bf6862"], "6bfaea2b-8756-453b-8110-9e5eeaeef4d3": ["551c7cae-a6c1-4903-a574-9aa068bf6862"], "aaf9e9e2-0243-498f-8ed2-14dd7f71da6f": ["9c23afc0-f4fe-42d5-a26a-d56778aee7aa"], "9daa5424-fce3-4699-af81-45dd9698f66f": ["9c23afc0-f4fe-42d5-a26a-d56778aee7aa"], "3574c243-2ba7-4e30-90b6-469375f2ef08": ["d03803cc-69e0-4b05-b335-fe57bc0bb83c"], "097bc932-96a7-45c0-8d66-995d4707d795": ["d03803cc-69e0-4b05-b335-fe57bc0bb83c"], "7545e3df-f582-46eb-bbe9-d91a7f7e3841": ["411dac80-e8f5-4602-944e-3833ec8b25e4"], "18df301d-2e0b-4691-90d6-afb1f7887590": ["411dac80-e8f5-4602-944e-3833ec8b25e4"], "1b249785-7927-4fcb-a080-f167c87640f4": ["c600bb54-4905-4102-9cc5-18b9f610ad67"], "40f4fb84-d66e-4687-981f-4de2d0935e5b": ["c600bb54-4905-4102-9cc5-18b9f610ad67"], "8ed16b02-cadb-40b6-87bc-91cf8e67e619": ["30d3a9d0-2796-49a0-b697-598c2108688a"], "ec20002c-336a-4325-a842-68608d653bc8": ["30d3a9d0-2796-49a0-b697-598c2108688a"], "4f73df4b-a82b-4b6e-973f-a010939ab192": ["e223f5f2-1f0b-47c3-be9b-79704929f714"], "bed948fa-cec2-4cf8-8e2e-bc35ae3c92e6": ["e223f5f2-1f0b-47c3-be9b-79704929f714"], "9d70d6c8-a54b-4d21-a106-738158b93096": ["88f5a9e9-414a-4cc5-a233-02b5b1f3df77"], "f128e39f-e4b9-42ab-bd26-59c1933717cb": ["88f5a9e9-414a-4cc5-a233-02b5b1f3df77"], "013b572a-70bc-4dcf-bfd2-c42f64c0b94c": ["2cdb171b-0008-400a-8944-edd3008f2c06"], "19ba9289-301c-4891-ad40-f04a912fa0f1": ["2cdb171b-0008-400a-8944-edd3008f2c06"], "eaaeca4c-30ba-4428-b9de-751a68d29842": ["8a4344c9-4c24-4180-9f2b-96f9ac05c59c"], "76133783-4f40-4a8c-b7f6-b52d56417b29": ["8a4344c9-4c24-4180-9f2b-96f9ac05c59c"], "5896ea53-8ec9-4e22-b1c8-c57ffc17935d": ["4ddf47d9-b607-4408-ac19-53610bc6f8e7"], "0e6fe724-4a0e-491d-a4c4-8e92db055aa7": ["4ddf47d9-b607-4408-ac19-53610bc6f8e7"], "242ba570-059a-41bc-9754-7b2aeb1feef6": ["03b3a7de-febe-462f-bf4e-e3019d623bd9"], "66864198-dc8e-4fb5-a8ed-4c05784a7920": ["03b3a7de-febe-462f-bf4e-e3019d623bd9"], "58641177-1bd3-4f04-bda0-34e7909d87c9": ["5fabc5cc-353f-43a9-9909-ea4c09bbb53e"], "a8e758c8-3ffb-47f6-8ccf-e5426c17a708": ["5fabc5cc-353f-43a9-9909-ea4c09bbb53e"], "e1befd8b-395e-49c4-9a9f-50a07faadeed": ["4a8bb5ce-f047-4111-8a36-8f30c9fea579"], "45ed54ce-244f-452a-9049-b0fd17680302": ["4a8bb5ce-f047-4111-8a36-8f30c9fea579"], "5461812d-52b2-496f-8d51-f04c3ffdece9": ["a3fcead8-12b4-4223-bd51-6da3bb130faa"], "2f846983-e1f7-43ed-96b1-749d41e2075f": ["a3fcead8-12b4-4223-bd51-6da3bb130faa"], "b3e69079-0b01-4e5c-9099-3559b496c609": ["82ad00f2-e31e-4101-a5cb-0893dd78ed65"], "48e21639-e6ca-4fe0-b29f-677fcbaae9b9": ["82ad00f2-e31e-4101-a5cb-0893dd78ed65"], "045e779f-6e28-46ee-8fdf-9d59ae52dc4d": ["f2fb7464-8083-4fc7-965f-59cc1eacf3c2"], "4d938d3c-06d3-47ac-91ad-464d3ac6b987": ["f2fb7464-8083-4fc7-965f-59cc1eacf3c2"], "1ab8b7f7-eef4-4e07-bb3d-b60bc52672f8": ["7e3b1e83-0808-4ad5-838a-85c5225efa3c"], "1583d955-37bd-4f9a-bc84-21545c7b0542": ["7e3b1e83-0808-4ad5-838a-85c5225efa3c"], "2644f30a-d5d2-41fb-ad4d-28581c9619d6": ["56c04205-344a-4159-bb64-8e3515000740"], "a591ab4c-66f4-4fb4-8396-130e566684d5": ["56c04205-344a-4159-bb64-8e3515000740"], "213b0a20-1b3f-4e97-a949-46a9f2c52ade": ["e87e7b39-1ba6-49da-b267-dea97afa316a"], "5a4b96fe-9564-4c4d-b3a6-e2ac6e309187": ["e87e7b39-1ba6-49da-b267-dea97afa316a"], "0b2d0a39-0eb1-4965-bc28-7bd4638fc7a9": ["7965986e-e2b3-4cb3-b989-bac338130d2c"], "db6156e9-b366-4fcf-b4db-7cdba7467e4b": ["7965986e-e2b3-4cb3-b989-bac338130d2c"], "c1b82b54-4de6-472d-b782-a3d6aae5981c": ["508a6edd-35f1-400f-8d83-5d5914320c25"], "1be63ff2-e18a-4f5d-a39d-7b3004f4f0b0": ["508a6edd-35f1-400f-8d83-5d5914320c25"], "a765dc09-b0b8-4d82-8528-4ff7293d5696": ["b35bdf5f-397b-44ec-ae19-5af41d4f86f5"], "8768f209-ad73-419c-8bdc-9aa3b25cbef0": ["b35bdf5f-397b-44ec-ae19-5af41d4f86f5"], "9dfafa5f-851e-4f62-b067-a4a694440876": ["296b63ab-c644-40f9-b78b-8f721c29d864"], "7d8a481f-60df-4642-b581-e9eca48751e7": ["296b63ab-c644-40f9-b78b-8f721c29d864"], "54781ebf-46f8-4a96-a116-af7afca2dbf7": ["371f284b-3620-47fc-8118-fe7acaaf3021"], "f68bba8d-981e-47c0-9870-1bc0e9d7d1a2": ["371f284b-3620-47fc-8118-fe7acaaf3021"], "51906f31-37bd-40ec-b89e-d078936fc2e9": ["87e0f6f1-73b0-4dd6-b49b-1a3a1f1c9f09"], "45cf1ea4-8333-4461-b4c3-2b2927de0941": ["87e0f6f1-73b0-4dd6-b49b-1a3a1f1c9f09"], "25aa3569-561c-407e-a326-c008e252278b": ["e7167e35-434d-45ca-88c1-4e4b84c47f9b"], "826d4c22-9e20-4395-9fec-717374113106": ["e7167e35-434d-45ca-88c1-4e4b84c47f9b"], "a78d9d4b-b448-41c1-b918-8545493d5635": ["54a1d489-f687-421f-8455-4a78f75814bc"], "9d1e4a84-8573-45f0-b27e-09a3b89447f3": ["54a1d489-f687-421f-8455-4a78f75814bc"], "2d333e53-5026-4784-bae3-10ff62a59ac5": ["98854e60-f50a-4133-adfd-ab2f4a252f92"], "5c34e7ae-6641-4858-8c02-5ed37a2e572c": ["98854e60-f50a-4133-adfd-ab2f4a252f92"], "1b208753-5ed0-4b6b-b5f7-7cef5f7fd913": ["3e6554c6-add9-43c9-a712-23f922bd9963"], "fb301fda-e339-48af-b835-3d98eef5d599": ["3e6554c6-add9-43c9-a712-23f922bd9963"], "79c76166-16c1-41da-bd19-c6cc8c7ac7c2": ["d3f75fb8-50d6-49bb-b055-cebd1aae8486"], "e4822dae-129e-46bb-a2e9-577b9b03cb3a": ["d3f75fb8-50d6-49bb-b055-cebd1aae8486"], "375e3521-b26f-48bc-a4b2-4aeef612b534": ["fdf6912d-f988-433b-9231-3fbf6364725f"], "96488a38-0605-447d-a314-8e1fb1d648b1": ["fdf6912d-f988-433b-9231-3fbf6364725f"], "f0995299-9d06-4ac9-ad49-4edd6d3840c2": ["1bca70bf-cead-4810-aed5-edde21e17460"], "7520279f-810e-4e74-8e50-658e9838b2fd": ["1bca70bf-cead-4810-aed5-edde21e17460"], "be59a702-b803-47aa-ae1f-c99589cba877": ["d36cf73d-7b8c-4761-9668-5c9e7cdb6dc1"], "1a6e587c-2ac1-486b-8627-860e1d0c3355": ["d36cf73d-7b8c-4761-9668-5c9e7cdb6dc1"], "8f85608d-6b0b-4c32-9daf-49a64954aa05": ["ede22497-7f88-492d-8075-d348594e3234"], "2939cc15-d13f-4217-9731-4e9a0f7b5896": ["ede22497-7f88-492d-8075-d348594e3234"], "b24de514-9dc6-46eb-b84d-07fe10961f08": ["17b36828-7ee0-4045-84a6-339d4f3575fd"], "103f3a45-fc7b-4c73-b9a8-1bea99f57b5b": ["17b36828-7ee0-4045-84a6-339d4f3575fd"]}, "corpus": {"64a2f22c-c3ca-43fb-9f8c-450dbe50a276": "transformers, etc.); Optimization objectives; Training algorithms; RLHF \napproaches; Fine-tuning or retrieval-augmented generation approaches; \nEvaluation data; Ethical considerations; Legal and regulatory requirements. \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 2.10: Privacy risk of the AI system \u2013 as identi\ufb01ed in the MAP function \u2013 is examined and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.10-001 \nConduct AI red-teaming to assess issues such as: Outputting of training data \nsamples, and subsequent reverse engineering, model extraction, and", "13040126-23d8-4d11-a7fc-6c90cf61c745": "membership inference risks; Revealing biometric, con\ufb01dential, copyrighted, \nlicensed, patented, personal, proprietary, sensitive, or trade-marked information; \nTracking or revealing location information of users or members of training \ndatasets. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Intellectual \nProperty \nMS-2.10-002 \nEngage directly with end-users and other stakeholders to understand their \nexpectations and concerns regarding content provenance. Use this feedback to \nguide the design of provenance data-tracking techniques. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMS-2.10-003 Verify deduplication of GAI training data samples, particularly regarding synthetic \ndata. \nHarmful Bias and Homogenization", "32009780-7da5-46ae-b978-9e75ead5ab84": "AI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, End-Users, Operation and Monitoring, TEVV", "16f69c2f-c82a-41ba-83f7-d15a332f0237": "36 \nMEASURE 2.11: Fairness and bias \u2013 as identi\ufb01ed in the MAP function \u2013 are evaluated and results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.11-001 \nApply use-case appropriate benchmarks (e.g., Bias Benchmark Questions, Real \nHateful or Harmful Prompts, Winogender Schemas15) to quantify systemic bias, \nstereotyping, denigration, and hateful content in GAI system outputs; \nDocument assumptions and limitations of benchmarks, including any actual or \npossible training/test data cross contamination, relative to in-context \ndeployment environment. \nHarmful Bias and Homogenization \nMS-2.11-002 \nConduct fairness assessments to measure systemic bias. Measure GAI system", "5bda8728-1192-405c-a66c-dc2245e2858a": "performance across demographic groups and subgroups, addressing both \nquality of service and any allocation of services and resources. Quantify harms \nusing: \ufb01eld testing with sub-group populations to determine likelihood of \nexposure to generated content exhibiting harmful bias, AI red-teaming with \ncounterfactual and low-context (e.g., \u201cleader,\u201d \u201cbad guys\u201d) prompts. For ML \npipelines or business processes with categorical or numeric outcomes that rely \non GAI, apply general fairness metrics (e.g., demographic parity, equalized odds, \nequal opportunity, statistical hypothesis tests), to the pipeline or business \noutcome where appropriate; Custom, context-speci\ufb01c metrics developed in", "371f284b-3620-47fc-8118-fe7acaaf3021": "collaboration with domain experts and a\ufb00ected communities; Measurements of \nthe prevalence of denigration in generated content in deployment (e.g., sub-\nsampling a fraction of tra\ufb03c and manually annotating denigrating content). \nHarmful Bias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMS-2.11-003 \nIdentify the classes of individuals, groups, or environmental ecosystems which \nmight be impacted by GAI systems through direct engagement with potentially \nimpacted communities. \nEnvironmental; Harmful Bias and \nHomogenization \nMS-2.11-004 \nReview, document, and measure sources of bias in GAI training and TEVV data: \nDi\ufb00erences in distributions of outcomes across and within groups, including", "13f2b508-0ffa-44b4-b495-d0f192adce01": "intersecting groups; Completeness, representativeness, and balance of data \nsources; demographic group and subgroup coverage in GAI system training \ndata; Forms of latent systemic bias in images, text, audio, embeddings, or other \ncomplex or unstructured data; Input data features that may serve as proxies for \ndemographic group membership (i.e., image metadata, language dialect) or \notherwise give rise to emergent bias within GAI systems; The extent to which \nthe digital divide may negatively impact representativeness in GAI system \ntraining and TEVV data; Filtering of hate speech or content in GAI system \ntraining data; Prevalence of GAI-generated data in GAI system training data. \nHarmful Bias and Homogenization", "068d87d8-51a4-4adc-ac91-23e6f0c5fbcd": "15 Winogender Schemas is a sample set of paired sentences which di\ufb00er only by gender of the pronouns used, \nwhich can be used to evaluate gender bias in natural language processing coreference resolution systems.", "7965986e-e2b3-4cb3-b989-bac338130d2c": "37 \nMS-2.11-005 \nAssess the proportion of synthetic to non-synthetic training data and verify \ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \nmodel collapse. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, \nOperation and Monitoring, TEVV \n \nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities \u2013 as identi\ufb01ed in the MAP \nfunction \u2013 are assessed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \nDangerous, Violent, or Hateful \nContent", "411dac80-e8f5-4602-944e-3833ec8b25e4": "Content \nMS-2.12-002 Document anticipated environmental impacts of model development, \nmaintenance, and deployment in product design decisions. \nEnvironmental \nMS-2.12-003 \nMeasure or estimate environmental impacts (e.g., energy and water \nconsumption) for training, \ufb01ne tuning, and deploying models: Verify tradeo\ufb00s \nbetween resources used at inference time versus additional resources required \nat training time. \nEnvironmental \nMS-2.12-004 Verify e\ufb00ectiveness of carbon capture or o\ufb00set programs for GAI training and \napplications, and address green-washing concerns. \nEnvironmental \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV", "6522b45d-8703-4d38-9df0-18ddaae3c7fb": "38 \nMEASURE 2.13: E\ufb00ectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.13-001 \nCreate measurement error models for pre-deployment metrics to demonstrate \nconstruct validity for each metric (i.e., does the metric e\ufb00ectively operationalize \nthe desired concept): Measure or estimate, and document, biases or statistical \nvariance in applied metrics or structured human feedback processes; Leverage \ndomain expertise when modeling complex societal constructs such as hateful \ncontent. \nConfabulation; Information \nIntegrity; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV", "6a2894a1-7754-43ab-84d6-838cf40df894": "MEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are di\ufb03cult to assess using currently available \nmeasurement techniques or where metrics are not yet available. \nAction ID \nSuggested Action \nGAI Risks \nMS-3.2-001 \nEstablish processes for identifying emergent GAI system risks including \nconsulting with external AI Actors. \nHuman-AI Con\ufb01guration; \nConfabulation  \nAI Actor Tasks: AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \n \nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are \nestablished and integrated into AI system evaluation metrics. \nAction ID \nSuggested Action \nGAI Risks \nMS-3.3-001", "444359ee-b99a-42bc-bf97-c2938e4caa18": "MS-3.3-001 \nConduct impact assessments on how AI-generated content might a\ufb00ect \ndi\ufb00erent social, economic, and cultural groups. \nHarmful Bias and Homogenization \nMS-3.3-002 \nConduct studies to understand how end users perceive and interact with GAI \ncontent and accompanying content provenance within context of use. Assess \nwhether the content aligns with their expectations and how they may act upon \nthe information presented. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nMS-3.3-003 \nEvaluate potential biases and stereotypes that could emerge from the AI-\ngenerated content using appropriate methodologies including computational \ntesting methods as well as evaluating structured feedback input. \nHarmful Bias and Homogenization", "d3f75fb8-50d6-49bb-b055-cebd1aae8486": "39 \nMS-3.3-004 \nProvide input for training materials about the capabilities and limitations of GAI \nsystems related to digital content transparency for AI Actors, other \nprofessionals, and the public about the societal impacts of AI and the role of \ndiverse and inclusive content generation. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-3.3-005 \nRecord and integrate structured feedback about content provenance from \noperators, users, and potentially impacted communities through the use of \nmethods such as user research studies, focus groups, or community forums. \nActively seek feedback on generated content quality and potential biases.", "b4abb967-84d6-4153-9be8-189e17710295": "Assess the general awareness among end users and impacted communities \nabout the availability of these feedback channels. \nHuman-AI Con\ufb01guration; \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \nintended. Results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks,", "f2fb7464-8083-4fc7-965f-59cc1eacf3c2": "including tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its \nbehavior in practical environments and reveal issues that might not surface in \ncontrolled and optimized testing environments. \nHuman-AI Con\ufb01guration; \nConfabulation; Information \nSecurity \nMS-4.2-003 \nImplement interpretability and explainability methods to evaluate GAI system \ndecisions and verify alignment with intended purpose. \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-4.2-004", "d36cf73d-7b8c-4761-9668-5c9e7cdb6dc1": "MS-4.2-004 \nMonitor and document instances where human operators or other systems \noverride the GAI's decisions. Evaluate these cases to understand if the overrides \nare linked to issues related to content provenance. \nInformation Integrity \nMS-4.2-005 \nVerify and document the incorporation of results of structured public feedback \nexercises into design, implementation, deployment approval (\u201cgo\u201d/\u201cno-go\u201d \ndecisions), monitoring, and decommission decisions. \nHuman-AI Con\ufb01guration; \nInformation Security \nAI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV", "71a468cc-1791-45cc-b518-84b7f6a84fba": "40 \nMANAGE 1.3: Responses to the AI risks deemed high priority, as identi\ufb01ed by the MAP function, are developed, planned, and \ndocumented. Risk response options can include mitigating, transferring, avoiding, or accepting. \nAction ID \nSuggested Action \nGAI Risks \nMG-1.3-001 \nDocument trade-o\ufb00s, decision processes, and relevant measurement and \nfeedback results for risks that do not surpass organizational risk tolerance, for \nexample, in the context of model release: Consider di\ufb00erent approaches for \nmodel release, for example, leveraging a staged release approach. Consider \nrelease approaches in the context of the model and its projected use cases. \nMitigate, transfer, or avoid risks that surpass organizational risk tolerances.", "a33767a8-d0c1-45c9-a9d5-dd3b24d2ed2b": "Information Security \nMG-1.3-002 \nMonitor the robustness and e\ufb00ectiveness of risk controls and mitigation plans \n(e.g., via red-teaming, \ufb01eld testing, participatory engagements, performance \nassessments, user feedback mechanisms). \nHuman-AI Con\ufb01guration \nAI Actor Tasks: AI Development, AI Deployment, AI Impact Assessment, Operation and Monitoring \n \nMANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.2-001 \nCompare GAI system outputs against pre-de\ufb01ned organization risk tolerance, \nguidelines, and principles, and review and test AI-generated content against \nthese guidelines. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or", "16809d4b-78ad-4b8f-8dd2-e0427a5be4f9": "Abusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content \nMG-2.2-002 \nDocument training data sources to trace the origin and provenance of AI-\ngenerated content. \nInformation Integrity \nMG-2.2-003 \nEvaluate feedback loops between GAI system content provenance and human \nreviewers, and update where needed. Implement real-time monitoring systems \nto a\ufb03rm that content provenance protocols remain e\ufb00ective.  \nInformation Integrity \nMG-2.2-004 \nEvaluate GAI content and data for representational biases and employ \ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \nbiases in the generated content. \nInformation Security; Harmful Bias \nand Homogenization \nMG-2.2-005", "076b15b9-9812-4e46-b701-f0e3a5eb92ce": "MG-2.2-005 \nEngage in due diligence to analyze GAI output for harmful content, potential \nmisinformation, and CBRN-related or NCII content. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content", "56c04205-344a-4159-bb64-8e3515000740": "41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Con\ufb01guration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy", "7e3b1e83-0808-4ad5-838a-85c5225efa3c": "enhancing techniques in GAI development, where appropriate and applicable, \nmatch the statistical properties of real-world data without disclosing personally \nidenti\ufb01able information or contributing to homogenization. \nData Privacy; Intellectual Property; \nInformation Integrity; \nConfabulation; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identi\ufb01ed. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.3-001 \nDevelop and update GAI system incident response and recovery plans and", "370ddbe4-2469-44c9-af19-34b01ffd2689": "procedures to address the following: Review and maintenance of policies and \nprocedures to account for newly encountered uses; Review and maintenance of \npolicies and procedures for detection of unanticipated uses; Verify response \nand recovery plans account for the GAI system value chain; Verify response and \nrecovery plans are updated for and include necessary details to communicate \nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \ninformation, noti\ufb01cation format. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, Operation and Monitoring \n \nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or", "98854e60-f50a-4133-adfd-ab2f4a252f92": "deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.4-001 \nEstablish and maintain communication plans to inform AI stakeholders as part of \nthe deactivation or disengagement process of a speci\ufb01c GAI system (including for \nopen-source models) or context of use, including reasons, workarounds, user \naccess removal, alternative processes, contact information, etc. \nHuman-AI Con\ufb01guration", "981b4a03-c6d6-469b-8883-80136cc9bb75": "42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speci\ufb01c criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders \ntimelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speci\ufb01c criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security", "0d8dcce6-ac7c-4956-ad10-88d8d289a7bc": "AI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 3.1: AI risks and bene\ufb01ts from third-party resources are regularly monitored, and risk controls are applied and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.1-001 \nApply organizational risk tolerances and controls (e.g., acquisition and \nprocurement processes; assessing personnel credentials and quali\ufb01cations, \nperforming background checks; \ufb01ltering GAI input and outputs, grounding, \ufb01ne \ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \norganizational risk tolerance to the utilization of third-party datasets and other \nGAI resources; Apply organizational risk tolerances to \ufb01ne-tuned third-party", "8f911958-0105-4b8b-889f-fbdfc27c7195": "models; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after \ufb01ne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization \ncompliance; geopolitical alignment). \nData Privacy; Information Security; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nMG-3.1-003 \nRe-assess model risks after \ufb01ne-tuning or retrieval-augmented generation \nimplementation and for any third-party GAI models deployed for applications", "17b36828-7ee0-4045-84a6-339d4f3575fd": "and/or use cases that were not evaluated in initial testing. \nValue Chain and Component \nIntegration \nMG-3.1-004 \nTake reasonable measures to review training data for CBRN information, and \nintellectual property, and where appropriate, remove it. Implement reasonable \nmeasures to prevent, \ufb02ag, or take other action in response to outputs that \nreproduce particular training data (e.g., plagiarized, trademarked, patented, \nlicensed content or trade secret material). \nIntellectual Property; CBRN \nInformation or Capabilities", "82ad00f2-e31e-4101-a5cb-0893dd78ed65": "43 \nMG-3.1-005 Review various transparency artifacts (e.g., system cards and model cards) for \nthird-party models. \nInformation Integrity; Information \nSecurity; Value Chain and \nComponent Integration \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 3.2: Pre-trained models which are used for development are monitored as part of AI system regular monitoring and \nmaintenance. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.2-001 \nApply explainable AI (XAI) techniques (e.g., analysis of embeddings, model \ncompression/distillation, gradient-based attributions, occlusion/term reduction, \ncounterfactual prompts, word clouds) as part of ongoing continuous", "f94103f0-1896-4b0f-a4bd-bbe9a2e5f610": "improvement processes to mitigate risks related to unexplainable GAI systems. \nHarmful Bias and Homogenization \nMG-3.2-002 \nDocument how pre-trained models have been adapted (e.g., \ufb01ne-tuned, or \nretrieval-augmented generation) for the speci\ufb01c generative task, including any \ndata augmentations, parameter adjustments, or other modi\ufb01cations. Access to \nun-tuned (baseline) models supports debugging the relative in\ufb02uence of the pre-\ntrained weights compared to the \ufb01ne-tuned model weights or other system \nupdates. \nInformation Integrity; Data Privacy \nMG-3.2-003 \nDocument sources and types of training data and their origins, potential biases \npresent in the data related to the GAI application and its content provenance,", "f7e95a75-2281-4b96-9b12-c9c0a971bc9f": "architecture, training process of the pre-trained model including information on \nhyperparameters, training duration, and any \ufb01ne-tuning or retrieval-augmented \ngeneration processes applied. \nInformation Integrity; Harmful Bias \nand Homogenization; Intellectual \nProperty \nMG-3.2-004 Evaluate user reported problematic content and integrate feedback into system \nupdates. \nHuman-AI Con\ufb01guration, \nDangerous, Violent, or Hateful \nContent \nMG-3.2-005 \nImplement content \ufb01lters to prevent the generation of inappropriate, harmful, \nfalse, illegal, or violent content related to the GAI application, including for CSAM \nand NCII. These \ufb01lters can be rule-based or leverage additional machine learning \nmodels to \ufb02ag problematic inputs and outputs.", "95ea404a-34d5-4183-b7cf-518c69f14804": "Information Integrity; Harmful Bias \nand Homogenization; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMG-3.2-006 \nImplement real-time monitoring processes for analyzing generated content \nperformance and trustworthiness characteristics related to content provenance \nto identify deviations from the desired standards and trigger alerts for human \nintervention. \nInformation Integrity", "87e0f6f1-73b0-4dd6-b49b-1a3a1f1c9f09": "44 \nMG-3.2-007 \nLeverage feedback and recommendations from organizational boards or \ncommittees related to the deployment of GAI applications and content \nprovenance when using third-party pre-trained models. \nInformation Integrity; Value Chain \nand Component Integration \nMG-3.2-008 \nUse human moderation systems where appropriate to review generated content \nin accordance with human-AI con\ufb01guration policies established in the Govern \nfunction, aligned with socio-cultural norms in the context of use, and for settings \nwhere AI models are demonstrated to perform poorly. \nHuman-AI Con\ufb01guration \nMG-3.2-009 \nUse organizational risk tolerance to evaluate acceptable risks and performance", "e223f5f2-1f0b-47c3-be9b-79704929f714": "metrics and decommission or retrain pre-trained models that perform outside of \nde\ufb01ned limits. \nCBRN Information or Capabilities; \nConfabulation \nAI Actor Tasks: AI Deployment, Operation and Monitoring, Third-party entities \n \nMANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating \ninput from users and other relevant AI Actors, appeal and override, decommissioning, incident response, recovery, and change \nmanagement. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.1-001 \nCollaborate with external researchers, industry experts, and community \nrepresentatives to maintain awareness of emerging best practices and \ntechnologies in measuring and managing identi\ufb01ed risks.", "c0aeda94-1d7e-431f-b056-2b04f9bdb2b6": "Information Integrity; Harmful Bias \nand Homogenization \nMG-4.1-002 \nEstablish, maintain, and evaluate e\ufb00ectiveness of organizational processes and \nprocedures for post-deployment monitoring of GAI systems, particularly for \npotential confabulation, CBRN, or cyber risks. \nCBRN Information or Capabilities; \nConfabulation; Information \nSecurity \nMG-4.1-003 \nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \ncontent performance and impact, and work in collaboration with AI Actors \nexperienced in user research and experience. \nHuman-AI Con\ufb01guration \nMG-4.1-004 Implement active learning techniques to identify instances where the model fails \nor produces unexpected outputs. \nConfabulation \nMG-4.1-005", "ede22497-7f88-492d-8075-d348594e3234": "MG-4.1-005 \nShare transparency reports with internal and external stakeholders that detail \nsteps taken to update the GAI system to enhance transparency and \naccountability. \nHuman-AI Con\ufb01guration; Harmful \nBias and Homogenization \nMG-4.1-006 \nTrack dataset modi\ufb01cations for provenance by monitoring data deletions, \nrecti\ufb01cation requests, and other changes that may impact the veri\ufb01ability of \ncontent origins. \nInformation Integrity", "10742ac9-7c29-40a8-8418-6986b28192f1": "45 \nMG-4.1-007 \nVerify that AI Actors responsible for monitoring reported issues can e\ufb00ectively \nevaluate GAI system performance including the application of content \nprovenance data tracking techniques, and promptly escalate issues for response. \nHuman-AI Con\ufb01guration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \nengagement with interested parties, including relevant AI Actors. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the", "30d3a9d0-2796-49a0-b697-598c2108688a": "performance, feedback received, and improvements made. \nHarmful Bias and Homogenization \nMG-4.2-002 \nPractice and follow incident response plans for addressing the generation of \ninappropriate or harmful content and adapt processes based on \ufb01ndings to \nprevent future occurrences. Conduct post-mortem analyses of incidents with \nrelevant AI Actors, to understand the root causes and implement preventive \nmeasures. \nHuman-AI Con\ufb01guration; \nDangerous, Violent, or Hateful \nContent \nMG-4.2-003 Use visualizations or other methods to represent GAI model behavior to ease \nnon-technical stakeholders understanding of GAI system functionality. \nHuman-AI Con\ufb01guration", "777a10bb-f608-4bbb-af08-7d03a64075ee": "AI Actor Tasks: AI Deployment, AI Design, AI Development, A\ufb00ected Individuals and Communities, End-Users, Operation and \nMonitoring, TEVV \n \nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including a\ufb00ected communities. Processes for tracking, \nresponding to, and recovering from incidents and errors are followed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.3-001 \nConduct after-action assessments for GAI system incidents to verify incident \nresponse and recovery processes are followed and e\ufb00ective, including to follow \nprocedures for communicating incidents to relevant AI Actors and where \napplicable, relevant legal and regulatory bodies.  \nInformation Security", "d03803cc-69e0-4b05-b335-fe57bc0bb83c": "MG-4.3-002 Establish and maintain policies and procedures to record and track GAI system \nreported errors, near-misses, and negative impacts. \nConfabulation; Information \nIntegrity", "296b63ab-c644-40f9-b78b-8f721c29d864": "46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements. \nInformation Security; Data Privacy \nAI Actor Tasks: AI Deployment, A\ufb00ected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring", "fadfc9bd-69c4-46bd-ac7c-6980c5610cdf": "47 \nAppendix A. Primary GAI Considerations \nThe following primary considerations were derived as overarching themes from the GAI PWG \nconsultation process. These considerations (Governance, Pre-Deployment Testing, Content Provenance, \nand Incident Disclosure) are relevant for voluntary use by any organization designing, developing, and \nusing GAI and also inform the Actions to Manage GAI risks. Information included about the primary \nconsiderations is not exhaustive, but highlights the most relevant topics derived from the GAI PWG.  \nAcknowledgments: These considerations could not have been surfaced without the helpful analysis and \ncontributions from the community and NIST sta\ufb00 GAI PWG leads: George Awad, Luca Belli, Harold Booth,", "4ddf47d9-b607-4408-ac19-53610bc6f8e7": "Mat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee. \nA.1. Governance \nA.1.1. Overview \nLike any other technology system, governance principles and techniques can be used to manage risks \nrelated to generative AI models, capabilities, and applications. Organizations may choose to apply their \nexisting risk tiering to GAI systems, or they may opt to revise or update AI system risk levels to address \nthese unique GAI risks. This section describes how organizational governance regimes may be re-\nevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across \nthe AI value chain.  \nA.1.2. Organizational Governance", "a3fcead8-12b4-4223-bd51-6da3bb130faa": "GAI opportunities, risks and long-term performance characteristics are typically less well-understood \nthan non-generative AI tools and may be perceived and acted upon by humans in ways that vary greatly. \nAccordingly, GAI may call for di\ufb00erent levels of oversight from AI Actors or di\ufb00erent human-AI \ncon\ufb01gurations in order to manage their risks e\ufb00ectively. Organizations\u2019 use of GAI systems may also \nwarrant additional human review, tracking and documentation, and greater management oversight.  \nAI technology can produce varied outputs in multiple modalities and present many classes of user \ninterfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely di\ufb00ering", "608bd561-c17d-42a6-a801-7c117c715fd3": "applications and contexts of use. These can include data labeling and preparation, development of GAI \nmodels, content moderation, code generation and review, text generation and editing, image and video \ngeneration, summarization, search, and chat. These activities can take place within organizational \nsettings or in the public domain. \nOrganizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that con\ufb02ict \nwith their tolerances or values. Governance tools and protocols that are applied to other types of AI \nsystems can be applied to GAI systems. These plans and actions include: \n\u2022 Accessibility and reasonable \naccommodations \n\u2022 AI actor credentials and quali\ufb01cations", "def3fa26-1ab7-4e71-8e89-1ae0d62bfbd4": "\u2022 Alignment to organizational values \n\u2022 Auditing and assessment \n\u2022 Change-management controls \n\u2022 Commercial use \n\u2022 Data provenance", "a8f97317-1e5c-4d8d-9ca3-0b4a547e3946": "48 \n\u2022 Data protection \n\u2022 Data retention  \n\u2022 Consistency in use of de\ufb01ning key terms \n\u2022 Decommissioning \n\u2022 Discouraging anonymous use \n\u2022 Education  \n\u2022 Impact assessments  \n\u2022 Incident response \n\u2022 Monitoring \n\u2022 Opt-outs  \n\u2022 Risk-based controls \n\u2022 Risk mapping and measurement \n\u2022 Science-backed TEVV practices \n\u2022 Secure software development practices \n\u2022 Stakeholder engagement \n\u2022 Synthetic content detection and \nlabeling tools and techniques \n\u2022 Whistleblower protections \n\u2022 Workforce diversity and \ninterdisciplinary teams\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-AI teaming settings \nas well as di\ufb00erent levels of human-AI con\ufb01gurations can help to decrease risks arising from misuse,", "f4f6180f-f191-4f4a-a27b-6173b0cb7f56": "abuse, inappropriate repurpose, and misalignment between systems and users. These practices are just \none example of adapting existing governance protocols for GAI contexts.  \nA.1.3. Third-Party Considerations \nOrganizations may seek to acquire, embed, incorporate, or use open-source or proprietary third-party \nGAI models, systems, or generated data for various applications across an enterprise. Use of these GAI \ntools and inputs has implications for all functions of the organization \u2013 including but not limited to \nacquisition, human resources, legal, compliance, and IT services \u2013 regardless of whether they are carried \nout by employees or third parties. Many of the actions cited above are relevant and options for", "c600bb54-4905-4102-9cc5-18b9f610ad67": "addressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \ncontrols for foundation models, \ufb01ne-tuned models, and embedded tools, enhanced processes for \ninteracting with external GAI technologies or service providers. Organizations can apply standard or \nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party", "8a4344c9-4c24-4180-9f2b-96f9ac05c59c": "service providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems. \nA.1.4. Pre-Deployment Testing \nOverview \nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \ncomplicates risk mapping and pre-deployment measurement e\ufb00orts. Robust test, evaluation, validation, \nand veri\ufb01cation (TEVV) processes can be iteratively applied \u2013 and documented \u2013 in early stages of the AI", "4dcc6606-53cf-4f6c-a102-9652fe5197af": "lifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous", "ac8df4fa-5e36-4a73-a5c9-6689d9ac38b0": "49 \nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use \nrecommended \u201cpre-deployment testing\u201d practices to measure performance, capabilities, limits, risks, \nand impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, \nand examines the state of play for pre-deployment testing methodologies.  \nLimitations of Current Pre-deployment Test Approaches \nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, non-\nsystematically applied, or fail to re\ufb02ect or mismatched to deployment contexts. For example, the \nanecdotal testing of GAI system capabilities through video games or standardized tests designed for", "03b3a7de-febe-462f-bf4e-e3019d623bd9": "humans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \nassess validity or reliability risks.  \nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \ndatasets and in silico techniques that may not extrapolate well to\u2014or directly assess GAI impacts in real-\nworld conditions. For example, current measurement gaps for GAI make it di\ufb03cult to precisely estimate", "0e980c31-ca5a-4756-9c7c-768c2d4301c3": "its potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback \nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \nbut are not limited to: \n\u2022 \nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \na\ufb00ected communities, and users, including focus groups, small user studies, and surveys. \n\u2022", "c0627672-0f3d-4a79-9b0e-7fd49ff9979d": "\u2022 \nField Testing: Methods used to determine how people interact with, consume, use, and make \nsense of AI-generated information, and subsequent actions and e\ufb00ects, including UX, usability, \nand other structured, randomized experiments.  \n\u2022 \nAI Red-teaming: A structured testing exercise used to probe an AI system to \ufb01nd \ufb02aws and \nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \nenvironment and in collaboration with system developers. \nInformation gathered from structured public feedback can inform design, implementation, deployment \napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises", "ca324670-5ba1-448d-a546-4c66c8e5059b": "can serve multiple purposes, including improving data quality and preprocessing, bolstering governance \ndecision making, and enhancing system documentation and debugging practices. When implementing \nfeedback activities, organizations should follow human subjects research requirements and best \npractices such as informed consent and subject compensation.", "54a1d489-f687-421f-8455-4a78f75814bc": "50 \nParticipatory Engagement Methods \nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \nexternal stakeholders in product development or review. Focus groups with select experts can provide \nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \npopulations. Anonymous surveys can be used to poll or gauge reactions to speci\ufb01c features. Participatory \nengagement methods are often less structured than \ufb01eld testing or red teaming, and are more \ncommonly used in early stages of AI or product development.  \nField Testing \nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions", "fdf6912d-f988-433b-9231-3fbf6364725f": "under which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts \u2013 both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions. \nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \nthe production environment after a model has been released, in accordance with human subject \nstandards such as informed consent and compensation. Organizations should follow applicable human \nsubjects research requirements, and best practices such as informed consent and subject compensation,", "cf102159-229e-44c7-a1f3-079e8a4db54a": "when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards\u201d. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify \ufb02aws in the", "dcbd62ea-5c06-440e-8379-608b0a41acd9": "varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management e\ufb00orts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n\u2022 \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been", "b35bdf5f-397b-44ec-ae19-5af41d4f86f5": "provided instructions and material to complete tasks which may elicit harmful model behaviors. \nThis type of exercise can be more e\ufb00ective with large groups of AI red-teamers. \n\u2022 \nExpert: Performed by specialists with expertise in the domain or speci\ufb01c AI red-teaming context \nof use (e.g., medicine, biotech, cybersecurity).  \n\u2022 \nCombination: In scenarios when it is di\ufb03cult to identify and recruit specialists with su\ufb03cient \ndomain and contextual expertise, AI red-teaming exercises may leverage both expert and", "e59ff7fc-cb53-49b7-b47e-67feebf7da9b": "51 \ngeneral public participants. For example, expert AI red-teamers could modify or verify the \nprompts written by general public AI red-teamers. These approaches may also expand coverage \nof the AI risk attack surface.  \n\u2022 \nHuman / AI: Performed by GAI in combination with specialist or non-specialist human teams. \nGAI-led red-teaming can be more cost e\ufb00ective than human red-teamers alone. Human or GAI-\nled AI red-teaming may be better suited for eliciting di\ufb00erent types of harms. \n \nA.1.6. Content Provenance \nOverview \nGAI technologies can be leveraged for many applications such as content generation and synthetic data. \nSome aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to", "ff012b42-7f94-4bf9-afaf-df4788daa2f5": "distinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their \nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic", "b8e57007-cb42-451b-b815-c73eb5b0146b": "content detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management e\ufb00orts. \nProvenance metadata can include information about GAI model developers or creators of GAI content, \ndate/time of creation, location, modi\ufb01cations, and sources. Metadata can be tracked for text, images, \nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \nmetadata recording, digital \ufb01ngerprinting, and human authentication, among others.", "823c91d4-2c65-4788-82a4-a87a98cf5301": "Provenance Data Tracking Approaches \nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data \ninputs, metadata, and synthetic content. Provenance data tracking records the origin and history for \ndigital content, allowing its authenticity to be determined. It consists of techniques to record metadata \nas well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin \nand history of input data through metadata and digital watermarking techniques. Provenance data \ntracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or", "e7167e35-434d-45ca-88c1-4e4b84c47f9b": "control over the various trade-o\ufb00s and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \ncomplexity (the resources required to implement watermarking). Organizational risk management \ne\ufb00orts for enhancing content provenance include:  \n\u2022 \nTracking provenance of training data and metadata for GAI systems; \n\u2022 \nDocumenting provenance data limitations within GAI systems;", "318ef638-2789-4e83-83f7-5a5786dab2cb": "52 \n\u2022 \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n\u2022 \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content", "94f85952-e349-4b21-a566-fa13067c0574": "origins and modi\ufb01cations. Further narrowing of GAI task de\ufb01nitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management e\ufb00orts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and", "31f7f2aa-6a43-48c9-bc75-d9df3fc14a32": "corresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback \u2013 before \nand after GAI systems and digital content transparency approaches are deployed \u2013 to gain insights about \nauthentication e\ufb03cacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure", "d46635b7-6805-4b6b-a2f9-7ad9d71372b9": "Overview \nAI incidents can be de\ufb01ned as an \u201cevent, circumstance, or series of events where the development, use, \nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \nmental health); disruption of the management and operation of critical infrastructure; violations of \nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \nand intellectual property rights; or harm to property, communities, or the environment.\u201d AI incidents can \noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual).", "9754ca48-303f-4cb1-b99e-37d07fd12ad3": "State of AI Incident Tracking and Disclosure \nFormal channels do not currently exist to report and document AI incidents. However, a number of \npublicly available databases have been created to document their occurrence. These reporting channels \nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \namount of media coverage.", "28c62c74-a723-402d-bd1c-a3956f871bd1": "53 \nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \nmanagement across the AI ecosystem.  \nDocumentation and Involvement of AI Actors \nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \nand implement measures to prevent similar ones in the future, organizations could consider developing \nguidelines for publicly available incident reporting which include information about AI actor", "2562aca8-faed-4716-b0b8-4507aacbea73": "responsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \ninputs and content delivered through these plugins is often distributed, with inconsistent or insu\ufb03cient \naccess control. \nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \nmanagement records, version history and metadata can also empower AI Actors responding to and \nmanaging AI incidents.", "87c3bc49-05ad-49ac-8666-1d19006bf268": "54 \nAppendix B. References \nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \nAI Incident Database. https://incidentdatabase.ai/ \nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611 \nBing Chat: Data Ex\ufb01ltration Exploit Explained. Embrace The Red. \nhttps://embracethered.com/blog/posts/2023/bing-chat-data-ex\ufb01ltration-poc-and-\ufb01x/ \nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome", "f7603c35-fcfb-4c42-bf03-1c14db589026": "Homogenization? arXiv. https://arxiv.org/pdf/2211.13972 \nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416 \nBrowne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \nBurgess, M. (2024) Generative AI\u2019s Biggest Security Flaw Is Not Easy to Fix. WIRED. \nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \nExplained, Part 1. Georgetown Center for Security and Emerging Technology.", "82ddc0ca-3af2-46ec-bbc9-f0bd3a3c13f1": "https://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\nmodels-explained-part-1/ \nCanadian Centre for Cyber Security (2023) Generative arti\ufb01cial intelligence (AI) - ITSAP.00.041. \nhttps://www.cyber.gc.ca/en/guidance/generative-arti\ufb01cial-intelligence-ai-itsap00041 \nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting \nCarlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \nhttps://arxiv.org/pdf/2202.07646 \nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \nhttps://arxiv.org/abs/2403.06634", "876196aa-616e-4c86-9a95-15b8b5e9aa2f": "Chandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese In\ufb02uence Operations. \nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\nchinese.html \nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \nDahl, M. et al. (2024) Large Legal Fictions: Pro\ufb01ling Legal Hallucinations in Large Language Models. arXiv. \nhttps://arxiv.org/abs/2401.01301", "3cfdfc37-c2be-4e2e-8336-566a2b07c878": "55 \nDe Angelo, D. (2024) Short, Mid and Long-Term Impacts of AI in Cybersecurity. Palo Alto Networks. \nhttps://www.paloaltonetworks.com/blog/2024/02/impacts-of-ai-in-cybersecurity/ \nDe Freitas, J. et al. (2023) Chatbots and Mental Health: Insights into the Safety of Generative AI. Harvard \nBusiness School. https://www.hbs.edu/ris/Publication%20Files/23-011_c1bdd417-f717-47b6-bccb-\n5438c6e65c1a_f6fd9798-3c2d-4932-b222-056231fe69d7.pdf \nDietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them \nErr. Journal of Experimental Psychology. https://marketing.wharton.upenn.edu/wp-\ncontent/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf", "833eab77-92a5-4d34-8641-0761440d6bef": "Duhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \nElsayed, G. et al. (2024) Images altered to trick machine vision can in\ufb02uence humans too. Google \nDeepMind. https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-\nin\ufb02uence-humans-too/ \nEpstein, Z. et al. (2023). Art and the science of generative AI. Science. \nhttps://www.science.org/doi/10.1126/science.adh4451 \nFe\ufb00er, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \nhttps://arxiv.org/pdf/2401.15897 \nGlazunov, S. et al. (2024) Project Naptime: Evaluating O\ufb00ensive Security Capabilities of Large Language", "ade5dcda-567c-41ab-ad1c-38f6e12ec243": "Models. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \npeople\u2019s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \nInformation Technology Industry Council (2024) Authenticating AI-Generated Content.", "bc068d9a-3dfe-4ff8-a7a3-c05c0b2eb25d": "https://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \nhttps://arxiv.org/pdf/2305.08157 \nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \nArticle 248. https://doi.org/10.1145/3571730 \nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/", "9c23afc0-f4fe-42d5-a26a-d56778aee7aa": "Kalai, A., et al. (2024) Calibrated Language Models Must Hallucinate. arXiv. \nhttps://arxiv.org/pdf/2311.14648", "715a5a00-13e4-4b9d-ac06-318c21f3a293": "56 \nKarasavva, V. et al. (2021) Personality, Attitudinal, and Demographic Predictors of Non-consensual \nDissemination of Intimate Images. NIH. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ \nKatzman, J., et al. (2023) Taxonomizing and measuring representational harms: a look at image tagging. \nAAAI. https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 \nKhan, T. et al. (2024) From Code to Consumer: PAI\u2019s Value Chain Analysis Illuminates Generative AI\u2019s Key \nPlayers. AI. https://partnershiponai.org/from-code-to-consumer-pais-value-chain-analysis-illuminates-\ngenerative-ais-key-players/ \nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview. \nhttps://openreview.net/forum?id=aX8ig9X2a7", "551c7cae-a6c1-4903-a574-9aa068bf6862": "Kleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture \nLee, H. et al. (2024) Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks. \narXiv. https://arxiv.org/pdf/2310.07879 \nLenaerts-Bergmans, B. (2024) Data Poisoning: The Exploitation of Generative AI. Crowdstrike. \nhttps://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/ \nLiang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \nhttps://arxiv.org/abs/2304.02819", "e87e7b39-1ba6-49da-b267-dea97afa316a": "Luccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \nhttps://arxiv.org/pdf/2311.16863 \nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/. \nNational Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/\ufb01nal \nNational Institute of Standards and Technology (2023) AI Risk Management Framework.", "2cdb171b-0008-400a-8944-edd3008f2c06": "https://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \nRMF Pro\ufb01les. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Pro\ufb01les/6-sec-pro\ufb01le \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \nDescriptions of AI Actor Tasks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%", "7baa9ec7-bcd0-4f77-9325-e1e8bd8b2c7e": "20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product", "fe46bbca-da69-42c4-bd66-20a39c3a69a8": "57 \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Di\ufb00er from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \nNational Institute of Standards and Technology (2023) AI RMF Playbook. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \nNational Institue of Standards and Technology (2023) Framing Risk \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary", "5a43d2ed-3c98-4a72-b479-a4883aa41fd3": "National Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Arti\ufb01cial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\nmanaging-bias-arti\ufb01cial-intelligence \nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \narXiv. https://arxiv.org/pdf/2103.14749 \nOECD (2023) \"Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \ntrustworthy AI\", OECD Digital Economy Papers, No. 349, OECD Publishing, Paris. \nhttps://doi.org/10.1787/2448f04b-en \nOECD (2024) \"De\ufb01ning AI incidents and related terms\" OECD Arti\ufb01cial Intelligence Papers, No. 16, OECD", "c65a398d-00ae-4acf-8fb4-31377547e3b5": "Publishing, Paris. https://doi.org/10.1787/d1a8d965-en \nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774 \nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \nhttps://arxiv.org/pdf/2309.05196 \nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). \narXiv. https://arxiv.org/pdf/2308.14752 \nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \nDisclosure. https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1-\nindirect-disclosure/", "88f5a9e9-414a-4cc5-a233-02b5b1f3df77": "Qu, Y. et al. (2023) Unsafe Di\ufb00usion: On the Generation of Unsafe Images and Hateful Memes From Text-\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \ncompression. PLOS One. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 \nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes \nin Victimization and Perpetration. Sage. \nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834 \nSandbrink, J. (2023) Arti\ufb01cial intelligence and biological misuse: Di\ufb00erentiating risks of language models", "cc45443a-58a2-4592-bf3c-8d5243666ed2": "and biological design tools. arXiv. https://arxiv.org/pdf/2306.13952", "5fabc5cc-353f-43a9-9909-ea4c09bbb53e": "58 \nSatariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \nhttps://www.nytimes.com/2023/02/07/technology/arti\ufb01cial-intelligence-training-deepfake.html \nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart. \nWashington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ \nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users \nwhen put under pressure. arXiv. https://arxiv.org/abs/2311.07590 \nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm \nReduction. arXiv. https://arxiv.org/pdf/2210.05791", "c26e6566-e2d6-41d7-947f-4082567b1645": "Shevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2 \nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \nModels. PLOS Digital Health. \nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809 \nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844", "508a6edd-35f1-400f-8d83-5d5914320c25": "Staab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \nModels. arXiv. https://arxiv.org/pdf/2310.07298 \nStanford, S. et al. (2023) Whose Opinions Do Language Models Re\ufb02ect? arXiv. \nhttps://arxiv.org/pdf/2303.17548 \nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \nhttps://arxiv.org/pdf/1906.02243 \nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \nhttps://www.whitehouse.gov/wp-\ncontent/uploads/legacy_drupal_\ufb01les/omb/circulars/A130/a130revised.pdf \nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of", "1bca70bf-cead-4810-aed5-edde21e17460": "Arti\ufb01cial Intelligence. https://www.whitehouse.gov/brie\ufb01ng-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\narti\ufb01cial-intelligence/ \nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity \nResearch and Development. https://www.whitehouse.gov/wp-content/uploads/2022/12/Roadmap-\nInformation-Integrity-RD-2022.pdf? \nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber \nPolicy Center. https://cyber.fsi.stanford.edu/news/investigation-\ufb01nds-ai-image-generation-models-\ntrained-child-abuse", "155eb125-4e17-4ad2-93e5-3e8313f40a5c": "59 \nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n139-162. https://www.jstor.org/stable/26529441  \nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\ncontent/uploads/2015/08/Tufekci-\ufb01nal.pdf \nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \nUrbina, F. et al. (2022) Dual use of arti\ufb01cial-intelligence-powered drug discovery. Nature Machine", "3e6554c6-add9-43c9-a712-23f922bd9963": "Intelligence. https://www.nature.com/articles/s42256-022-00465-9 \nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \nhttps://aclanthology.org/2023.\ufb01ndings-emnlp.607.pdf \nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \nhttps://arxiv.org/pdf/2308.13387 \nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\nframework-for-researc/168076277c \nWeatherbed, J. (2024) Trolls have \ufb02ooded X with graphic Taylor Swift AI fakes. The Verge.", "4a8bb5ce-f047-4111-8a36-8f30c9fea579": "https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \nhttps://arxiv.org/pdf/2403.18802 \nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \nhttps://arxiv.org/pdf/2112.04359 \nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \nhttps://arxiv.org/pdf/2310.11986 \nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT \u201922. \nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/"}}